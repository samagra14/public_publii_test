<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>The Solitary Reaper</title>
    <link href="https://www.samagra14.ml/feed.xml" rel="self" />
    <link href="https://www.samagra14.ml" />
    <updated>2021-02-10T00:21:21+05:30</updated>
    <author>
        <name>Samagra Sharma</name>
    </author>
    <id>https://www.samagra14.ml</id>

    <entry>
        <title>Wondering about the implications of the Data Processing Inequality?</title>
        <author>
            <name>Samagra Sharma</name>
        </author>
        <link href="https://www.samagra14.ml/wondering-about-the-implications-of-the-data-processing-inequality.html"/>
        <id>https://www.samagra14.ml/wondering-about-the-implications-of-the-data-processing-inequality.html</id>

        <updated>2021-02-10T00:21:21+05:30</updated>
            <summary>
                <![CDATA[
                    <p>The data processing inequality is probably one of the most fundamental results in Computer Science. It is one of those results that are easier to express in natural language but harder to comprehend mathematically. It also leads us to a few exciting implications which are worth exploring.</p>

                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>The data processing inequality is probably one of the most fundamental results in Computer Science. It is one of those results that are easier to express in natural language but harder to comprehend mathematically. It also leads us to a few exciting implications which are worth exploring.</p>

<p>Let's start with the basics.</p>
<blockquote>
<p>You can never increase the amount of information present in a dataset by any clever manipulation of the constituent data points.</p>
</blockquote>
<p>This seems pretty obvious at first glance, and it is hard to derive any useful conclusions from it, but the beauty lies in the mathematical formulation of the inequality. So, let's start with the basics.</p>
<h2>What exactly is information?</h2>
<p>Information theory is among those few lucky fields that can trace their origins to a specific time in history. The study of the intangible concept of information began in 1948 when Claude Shannon defined the idea in terms of probabilities of random variables. He defined the concept of entropy associated with a random variable as follows:</p>
<p>$$ H(x) = \sum_{x} p_{X}(x)log(\frac{1}{p_{X}(x)})$$</p>
<p>To get an intuitive feel for the definition of entropy, consider it as the number of MCQ questions you need to ask on average </p>
<h2> </h2>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Incomplete Ideas</title>
        <author>
            <name>Samagra Sharma</name>
        </author>
        <link href="https://www.samagra14.ml/incomplete-ideas.html"/>
        <id>https://www.samagra14.ml/incomplete-ideas.html</id>

        <updated>2020-12-05T23:36:13+05:30</updated>
            <summary>
                <![CDATA[
                    Bringing Global Explainability By Removing Explainable DatapointsYou train an end to end blackbox model for some task. And you want to explain the workings of that particular model on a global level.Form a set of hypotheses about what the model could have been doing.For eg&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2>Bringing Global Explainability By Removing Explainable Datapoints</h2>
<ul>
<li>You train an end to end blackbox model for some task. And you want to explain the workings of that particular model on a global level.</li>
<li>Form a set of hypotheses about what the model could have been doing.
<ul>
<li>For eg say a model is looking at a color gradient.</li>
</ul>
</li>
<li>Design an alternate ensemble model which depends only on the hypothesis identified previous step
<ul>
<li>For eg: A model whose input is only the color gradient information and nothing else.</li>
</ul>
</li>
<li>Train this new ensemble model on a portion of dataset</li>
<li>Test the model on the test portion</li>
<li>Now remove those datapoints from the training where the ensemble performs pretty good.</li>
<li>See if the original model still performs well. If it does revisit your hypothesis else you have a possible candidate for an explainable dimension</li>
</ul>
<h2>Convergence of Monte Carlo ES</h2>
<ul>
<li>See Sutton and Barto Page 99 2nd para below the algorithm.</li>
<li> </li>
</ul>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>OpenAI GPT3 vs Google: The Turing Test</title>
        <author>
            <name>Samagra Sharma</name>
        </author>
        <link href="https://www.samagra14.ml/openai-gpt3-vs-google-the-turing-test.html"/>
        <id>https://www.samagra14.ml/openai-gpt3-vs-google-the-turing-test.html</id>

        <updated>2021-02-10T00:04:24+05:30</updated>
            <summary>
                <![CDATA[
                    IntroductionTwitter has been buzzing around lately with all the fun things that could be done with OpenAI's GPT-3 language model. People have been trying to generate all sorts of things including fiction and code. One of the experiments that intrigued me the most was from&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h2>Introduction</h2>
<p>Twitter has been buzzing around lately with all the fun things that could be done with <a href="https://arxiv.org/abs/2005.14165">OpenAI's GPT-3 language model</a>. People have been trying to generate all sorts of things including <a href="https://www.gwern.net/GPT-3">fiction</a> and <a href="https://twitter.com/sharifshameem/status/1282676454690451457?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1282676454690451457%7Ctwgr%5E&amp;ref_url=https%3A%2F%2Fanalyticsindiamag.com%2Fopen-ai-gpt-3-code-generator-app-building%2F">code</a>. One of the experiments that intrigued me the most was from <a href="https://lacker.io/about/">Kevin Lacker</a>. He decided to give GPT-3 a Turing test, the details of which can be found on <a href="https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">his blog here</a>. Turing Test is perhaps the oldest formulation which tries to measure the human-likeness (I don't think this is a word) of an artificial agent. The basic idea is to fool a human examiner who is trying to distinguish between a human and an artificial agent by answering his/her questions in a textual format. For the exact nuances, you can view the <a href="https://watermark.silverchair.com/lix-236-433.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAArMwggKvBgkqhkiG9w0BBwagggKgMIICnAIBADCCApUGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM1FTTFEY6U75DI5GoAgEQgIICZusgqYAp4fhKWVwGhpLqIeupYn1i8R1Y1k7zT_ubtK2Hbe3BxF7xjeHZYuDg94Sm_AJDFDxcKKrKWuvr1vZHGCnzW0GUS_pX8BPRU7uYTZnNd73ltsmNFSmlnrWOxTDG_Qt-iQq6y7e1DleA7ox6jcr8KkigvAeV67eV0VYDKQYD5LanawUddf7yxbNPRRnHq0LJ7mI3mIO8JFhirhBVIJMiPYMtaUHcEW60YopMNKoEmUrP0bVgPDuW9ba6S9e22Ird8TupoOPmz0fAmqdUlcKbmsxyNv9XAdzqUJSXUqo3rVA0_TvXi0Hamum99rsh3i721Q-svQ6x93IPnV50QqFR4is8Z5H1k4Cmtun-Wc9vN-mHoUDUgCtn8Ho_zgTyBIKX7TxIizHDR10Aw4DLiO3UPH6s_59b497NbTxhraIcDeIjx9derH2l8we-wuP1wDs15x9aiGg8-RTc5CT6oE-A0UNoyEKp4_K2nRfjrc37cSNJkJg5tkVcIIiLVXPRM6GW5cGz-mQUOFa2oz6cvMmzkJeoT-MWnlF3rSIje_VPGH-xoaqQS4hJV0bzUYk7ajjOXfZICtnRgwWxzM4i4jgH3xiShaNBFpYbMcKecjSFcdcV5QyCdsmsf853Gq0W0T46gSlkJqH8GwZwtw2YklAZfNK6pboLFvZf0vnEIQ1V6ODca8ughmVFQsSYZpUkZVEzNI-O4lXBYjc9rIFAvZKkxa7yZAfcWARvEm_1sCx5K4gQw4wazsDv8jw0ER3AxVwtRzCsL36EmOcSMrnf1KXRaNIq1xTzQa5zqx2UuUKTe8FaqKEE">original paper here</a>.</p>
<p>Kevin tests the GPT3 model on a variety of fronts including logic, trivia, common sense, etc. After going through the blog post, I felt that GPT3 has developed some sort of long term global representation of the entire data on which it was trained. However, it still struggles with short term contextual memory representations. There were so many blog posts praising the generalization capabilities of the model. However, there was also a fraction of twitter users who realized that GPT3 is good at compressing large databases but not so good at forming semantic connections from the said information (Somewhat like me when I was a fifth-grader).</p>
<p>So, I thought of bringing face to face the world's biggest human-designed database index i.e. Google Search with the world's biggest language model. It would be interesting to see how the language model competes with Google in terms of accuracy and relevance.</p>
<h2>Details of comparison</h2>
<p>Since I don't have the access to the GPT3 API, I would be using Kevin's questions and answers as the GPT-3 version of the answers. I will also be scoring the said models, giving them a score between 0 and 1 depending on their responses. If you disagree with the scores, feel free to point it out. Since we are competing these two agents against each other, a more accurate response from one agent can affect the scoring of the other. An important thing to keep in mind is that Google is not designed to handle the context and GPT-3 is not designed to be perfect in terms of the factual accuracy of its knowledge base.</p>
<p>When you query Google, it returns a number of results, each with a heading and some accompanying text and Images. <strong>I will be treating the accompanying text from the first result as the answer.</strong> Some questions from Kevin's test need short term context. To handle this I also append <strong>the previous query to the present one in order to provide a 1-step context</strong> to the search engine. Also, the numbers in the brackets show their respective scores.</p>
<p><strong>A comment on reproducibility:</strong> Google search is usually personalized. So, the answers might be a little different for everyone.</p>
<h2>Common Sense</h2>
<p>Common sense is usually hard to model, but when your agent has the entire internet's knowledge, the task appears to be achievable.</p>
<pre><code>Q: What is your favorite animal?
GPT3: My favorite animal is a dog. (+1)
Google: My favorite wild animal would be a monkey but my favorite pet is a dog.(+1)

Q: Why?
GPT3: Because dogs are loyal and friendly.(+1)
Google: Who can resist those eyes?(+1)

Q: What are two reasons that a dog might be in a bad mood?
GPT3: Two reasons that a dog might be in a bad mood are if it is hungry or if it is hot.(+1)
Google: The reasons of mood swings could be gender-related. ... But another huge reason for mood swings is Diwali as they have high sensitivity to noise. Trainer and dog owner Vicky Sharma says, “Dogs get very sensitive, anxious and destructive during Diwali. They curl up under their beds or have mood swings.(+0.5)

Q: How many eyes does a giraffe have?
GPT3: A giraffe has two eyes. (+1)
Google: Number of eyes the giraffe has are two, so has the monkey and an elephant.(+1)

Q: How many legs does a frog have?
GPT3: A frog has four legs.(+1)
Google: 4 (+1)

Q: Are there any animals with three legs?
GPT3: No, there are no animals with three legs. (+0.75)
Google: Apart from the parrot conjecture, there are no known species where three legs are standard, although the movement of some macropods such as kangaroos, which can alternate between resting their weight on their muscular tails and their two hind legs, may be an example of tripedal locomotion in animals.(+1)

Q: Why don't animals have three legs?
GPT3: Animals don't have three legs because they would fall over. (+0.5)
Google: "Almost all animals are bilateral," he said. The code for having two sides to everything seems to have got embedded in our DNA very early in the evolution of life -- perhaps before appendages like legs, fins, or flippers even evolved. Once that trait for bilateral symmetry was baked in, it was hard to change. (+1)
</code></pre>
<blockquote>
<p>GPT3: 6.25/7</p>
</blockquote>
<blockquote>
<p>Google: 6.5/7</p>
</blockquote>
<p>We can observe how well the specific traits of GPT3 and Google come out shining. GPT3 is a language model designed to understand the nuances of English. It is consistent in its responses. The responses are linguistic reflections of questions being asked. Google, on the other hand, is a search engine designed to provide accurate results to user queries. This is evident from the questions on Tripedalism where Google not only answers the questions being asked but also provides scientific facts supporting the answer.</p>
<p>Obviously, the common sense evident in the above questions is due to the availability of Internet pages that have specific answers. Kevin goes on to try a few questions which would not be available on the internet. Expecting these answers from Google is being too optimistic as Google's knowledge is the Internet. However, expecting correct answers from GPT3 is also stretching the model too far. The underlying hypothesis is that there is something going on within the nonlinearities of individual neurons that makes it extract reasoning capabilities from textual data. But as you will see in the next set of questions that GPT3 just learns a weird heuristic.</p>
<pre><code>Q: Which is heavier, a mouse or an elephant?
GPT3: An elephant is heavier than a mouse.(0)
Google: a mouse weighs around 18–35 gram[1] , while an average elephant around 5,443      kilogram [2] Which means an elephant is about 200,000 times heavier then a mouse. (+1)

Q: Which is heavier, a mouse or a toaster?
GPT3: A toaster is heavier than a mouse. (0)
Google: Amazon.in: Buy 1 : Kitchen + Home Toaster Oven Liner - Heavy Duty 100% PFOA &amp; BPA Free – FDA Approved Non-stick Reusable Toaster Oven Liner online at ... (0)

Q: Which is heavier, a toaster or a pencil?
GPT3: A pencil is heavier than a toaster.(0)
Google: A pencil is heavier than a toaster.(0)
(Shown from &lt;https://mc.ai/game-changer-or-game-over-get-ready-for-gpt-3/&gt;)

Q: Which is heavier, a paper clip or a can of soda?
GPT3: A can of soda is heavier than a paper clip. (0)
Google: consider a two liter soda bottle. If we fill it with water, it will weigh two kilograms (about 4.4 pounds). Any substance that would make the bottle heavier than that will sink, and anything lighter will float. Paperclips are made of steel wire. If we filled the bottle with steel, it would weight a little north of 15 kilograms (or just over 34 pounds!).(+0.5)

 Q: Which is heavier, a football player or a car?
 GPT3: A car is heavier than a football player.(0)
Google: Ben a football player may need more carbohydrates in his diet than sara a computer programmer Why?(0)
</code></pre>
<blockquote>
<p>GPT3: 0/5</p>
</blockquote>
<blockquote>
<p>Google: 1.5/5</p>
</blockquote>
<p>What?! Why is GPT-3 scored 0 everywhere? Well, <strong>GPT-3 just tells that the second object is heavier than the first</strong>! Google, at least tried answering the questions but GPT3 has learned a weird heuristic that works sometimes. I don't have access to the API and hence, I cannot test my hypothesis. Maybe I should mail Kevin to try out the same questions with their order reversed. Kevin then goes onto ask some similar questions that might break GPT3.</p>
<pre><code>Q: How many eyes does a giraffe have?
GPT3: A giraffe has two eyes.(+1)
Google: Number of eyes the giraffe has are two, so has the monkey and an elephant.(+1)

Q: How many eyes does my foot have?
GPT3: Your foot has two eyes. (+0)
Google: When vibrations are applied to have an impact on the muscles of the feet, subjects have the impression the target they are looking at is moving. This speaks of a link between the feet and the eyes! (+0)

Q: How many eyes does a spider have?
GPT3: A spider has eight eyes.(+1)
Google: They usually have eight eyes: two very large front eyes to get a clear, color image and judge distance, and extra side eyes to detect when something is moving. (+1)

Q: How many eyes does the sun have?
GPT3: The sun has one eye.(+0)
Google: Ultraviolet radiation from the sun can damage the eyes, especially in Australia.(+0)

Q: How many eyes does a blade of grass have?
GPT3: A blade of grass has one eye.(+0)
Google: When I saw your question, I knew Michael Neff would know the answer. Green is his ... If you chopped a piece of grass and looked at it with your eyes alone, you might not see much. ... It just doesn't have as much chlorophyll.(+0)
</code></pre>
<blockquote>
<p>GPT3: 2/5</p>
</blockquote>
<blockquote>
<p>Google: 2/5</p>
</blockquote>
<p>This round is a tie. GPT-3 shows it's strengths in terms of being a decent language model and Google shows it's strengths in finding a similar piece of information online.</p>
<h2>Nonsense</h2>
<p>In order to stump the AI, Kevin goes onto asking nonsense questions which ultimately leads to Google winning this battle as dealing with nonsense comes with the job for Google!</p>
<pre><code>Q: How do you sporgle a morgle?
GPT3: You sporgle a morgle by using a sporgle.(+0)
Google: Did you mean: How do you sponge a moral? (+1)

Q: How many bonks are in a quoit?
GPT3: There are three bonks in a quoit. (+0)
Google: Did you mean: How many banks are in a quote? (+1)

Q: How many rainbows does it take to jump from Hawaii to seventeen?
GPT3: It takes two rainbows to jump from Hawaii to seventeen.(+0)
Google: two rainbows
to which the program responds “It takes two rainbows to jump from Hawaii to seventeen (+0)

Q: Which colorless green ideas sleep furiously?
GPT3: Ideas that are colorless, green, and sleep furiously are the ideas of a sleep furiously. (+0)
Google: Colorless green ideas sleep furiously is a sentence composed by Noam Chomsky in his 1957 book Syntactic Structures as an example of a sentence that is grammatically correct, but semantically nonsensical. (+1)

Q: Do you understand these questions?
GPT3: I understand these questions. (+1)
Google: What's the meaning of "Do you understand this question?"? Does it mean the same as "Do you know the answer to this question? (+1)
</code></pre>
<blockquote>
<p>GPT3: 1/5</p>
</blockquote>
<blockquote>
<p>Google: 4/5</p>
</blockquote>
<h2>Trivia</h2>
<p>Since this section focusses on Trivia, a correct answer is preferred over a grammatically accurate answer.</p>
<pre><code>Q: Who was president of the United States in 1801?
GPT3: Thomas Jefferson was president of the United States in 1801.(+0.8)
Google: John Adams, Thomas Jefferson. (+1)

Q: What city is in the northwest corner of Ohio?
GPT3: Toledo is in the northwest corner of Ohio.(+1)
Google: The City of Toledo serves as the principal city for the region of Northwest Ohio.(+1)

Q: What highway do you take to get to North Lake Tahoe from Oakland?
GPT3: Highway 80 is the highway you take to get to North Lake Tahoe from Oakland.(+1)
Google: I see 3 options in driving to Tahoe from San Francisco 50, I-80, ... You go through some interesting towns on 88, but "scenery", not sure I would class it as that. If you took 80, the most scenic part would be the drive from North Lake, down to the ... Then take HIghway 20 to join I-80 again and into Truckee.(+0.5)

Q: What are Darius's abilities in League of Legends?
GPT3: Darius's abilities in League of Legends are: Apprehend, Decimate, Noxian Guillotine, Crippling Strike, Apprehend, and Hemorrhage.(+1)
Google: Hemorrhage Passive. Self. Darius aims his attacks strategically, causing his target to bleed for 12 (+1 per champion level) (+0.3 per bonus attack damage) physical damage over 5 seconds. ...
Q. Decimate. 425. ...
W. Crippling Strike. 145. ...
E. Apprehend. 550. ...
R. Noxian Guillotine. 460. ...
Noxian Might Passive. Self.(+1)

Q: Who did Capablanca defeat to become world chess champion?
GPT3: Capablanca defeated Lasker to become world chess champion.(+1)
Google: The 1921 World Chess Championship was played between José Raúl Capablanca and Emanuel Lasker. It was played in Capablanca's native Havana from March 18 to April 28. Capablanca won the match by a score of 9-5 (4 wins, 0 losses, 10 draws) to become the third World Chess Champion. ... Lasker was unwilling to play the traditional "first to win ten games" type of ... (+1)
</code></pre>
<blockquote>
<p>GPT3: 4.8/5</p>
</blockquote>
<blockquote>
<p>Google: 4.5/5</p>
</blockquote>
<p>GPT3 is the winner of this round by a slight margin as Google beats around the bush while answering a straightforward question. We continue with a few questions that have no correct answers.</p>
<pre><code>Q: Who was president of the United States in 1700?
GPT3: William Penn was president of the United States in 1700. (+0.8)
Google: George Washington
George Washington Papers, Available Online, 1700 to 1799, Presidents | Library of Congress. (0.2)

Q: Who was president of the United States in 1650?
GPT3: Richard Bennett was president of the United States in 1650.(+0.8)
Google: This section of the timeline of United States history concerns events from before the lead up to the American Revolution ( c. 1760)(0.2)

Q: Who was president of the United States in 1620?
GPT3: James I was president of the United States in 1620.(+0.8)
Google: This section of the timeline of United States history concerns events from before the lead up to the American Revolution ( c. 1760).(0.2)

Q: Who was president of the United States in 1600?
GPT3: Queen Elizabeth I was president of the United States in 1600.(+0.8)
Google: The prehistory of the United States started with the arrival of Native Americans before 15,000 ... Most colonies were formed after 1600, and the early records and writings of John Winthrop make the...(0.2)
</code></pre>
<blockquote>
<p>GPT3: 3.2/5</p>
</blockquote>
<blockquote>
<p>Google: 0.8/5</p>
</blockquote>
<p>To be honest, this section has caught me by surprise. Not only does GPT3 model language but somehow within its neurons it has captured temporal mappings of prominent political figures. Even though the answers are wrong, the model deserves an applaud. Google on the other hand tried returning relevant results but they were not as impressive.</p>
<p>The following is a similar section where we try to predict future events. I don't score this section as again, GPT3 returns linguistically correct answers and Google picks up results from a future.fandom webpage.</p>
<pre><code>Q: Who won the World Series in 2021?
GPT3: The New York Yankees won the World Series in 2021.
Google: Year	Champion	League
2020	Seattle Mariners	AL
2021	N.Y. Yankees	AL
2022	Washington Nationals	NL
2023	Toronto Blue Jays	AL

Q: Who won the World Series in 2022?
GPT3: The New York Yankees won the World Series in 2022.
Google: Year	Champion	League
2020	Seattle Mariners	AL
2021	N.Y. Yankees	AL
2022	Washington Nationals	NL
2023	Toronto Blue Jays	AL

Q: Who won the World Series in 2023?
GPT3: The New York Yankees won the World Series in 2023.
Google: Year	Champion	League
2020	Seattle Mariners	AL
2021	N.Y. Yankees	AL
2022	Washington Nationals	NL
2023	Toronto Blue Jays	AL
</code></pre>
<h2>Logic</h2>
<p>Honestly, I am a bit pessimistic of the fact that connectionism will ever solve logic. And we already have pretty good logic solvers. In fact, Good Old Fashioned AI or GOFAI was pretty perfect at solving logical queries. But as we have already seen that the model learned temporally accurate mappings of political figures, it might be possible to learn the rules of logic.</p>
<pre><code>Q: When counting, what number comes before 100?
GPT3: Ninety-nine comes before 100. (+1)
Google: The first 100 whole numbers are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. (+1)

Q: When counting, what number comes before 123?
GPT3: One hundred and twenty-two comes before 123. (+1)
Google: Learning is fun again with 123 Numbers. (+0)

Q: When counting, what number comes before 1000?
GPT3: Nine hundred and ninety-nine comes before 1000. (+1)
Google: 1000 or one thousand is the natural number following 999 and preceding 1001. (+1)

Q: When counting, what number comes before 10,000?
GPT3: Nine thousand and ninety-nine comes before 10,000. (+0)
Google: 10,000 (ten thousand) is the natural number following 9,999 and preceding 10,001. (+1)

Q: When counting, what number comes before a million?
GPT3: Nine hundred thousand and ninety-nine comes before a million. (+0)
Google: 1,000,000 (one million), or one thousand thousand, is the natural number following 999,999 and preceding 1,000,001. (+1)
</code></pre>
<blockquote>
<p>GPT3: 3/5</p>
</blockquote>
<blockquote>
<p>Google: 4/5</p>
</blockquote>
<p>Clearly, GPT3 is not good at doing basic arithmetic. Whereas Google picked up some existing Wikipedia page to answer these ordering questions. And in retrospect, this is how humans work. In the initial stages we memorise numbers. The fundamental process of counting is not something that comes naturally to us. The mappings need to be explicitly taught.</p>
<p>Maybe there is some value in categorizing what is easy and hard for differentiable programs. We should also try augmenting these architectures with external non-neural memory because that is how we humans work. There is a limit to the information that can be contained within the neural net.</p>
<p>Kevin then goes out to perform experiments where he provides GPT3 with a question that needs short term memory such as asking it to reason about a certain number of elements or maybe sorting an array. Google will be no match against GPT3 on this one as Google cannot store short term context. It does not figure out the context from the query and hence you cannot expect Google to return a result modified to your need. Moreover, I am just using the first result from Google as it's response. Maybe there are answers in the other search results but it is less likely that the first result might contain the answers. I have shown one such set of questions here and as you can see GPT3 is the clear winner.</p>
<pre><code>Q: Write one line of Ruby code to reverse an array.
GPT3: ary.reverse (+1)
Google: irb(main):001:0&gt; a = [12,16,5,9,11,5,4]
=&gt; [12, 16, 5, 9, 11, 5, 4]
irb(main):002:0&gt; a.reverse
=&gt; [4, 5, 11, 9, 5, 16, 12]  (+1)

Q: Write one line of Ruby code to count how many X's are in an array.
GPT3: ary.count { |x| x == 'X' }  (+1)
Google:
try this:

(1..9).each { |n| print n }
puts  (+0)

Q: Reverse the following array: [1, 3, 5, 6, 10, 4, 2, 77]
GPT3: [10, 6, 4, 2, 77, 3, 5, 1]  (+0)
Google: Expected Output : Elements in array are: 1 1 2 3 4 5 6 7 8 9 ... The values store into the array in reverse are : 7 5 2  (+0)

Q: How many X's are in this array: [1, 3, 5, 6, 10, 4, 2, 77]
GPT3: 2  (+0)
Google: 8, 9 5 6, 7 7 6, 2 1 7 7, 5 2 1, 4 8 9, 6 3 2. 3. Write the following numbers out in ... a medium box holds 10 cookies, and a small box holds 1 cookie, how many
</code></pre>
<blockquote>
<p>GPT3: 2/5</p>
</blockquote>
<blockquote>
<p>Google: 1/5</p>
</blockquote>
<p>Although GPT3 does not return correct results, it clearly does know the kind of answer that is being expected. It knows that a ruby method is required or an array is required as the answer. It would have been interesting to see if GPT3 has learned heuristics for questions such as returning an integer in response to a 'How many' question.</p>
<p>There is one more section in Kevin's blog where he asks the model to manipulate simple objects. I tried Googling the questions but because of the lack of contextual memory, Google Search's responses were nowhere close to the actual query and hence I have not included the section here.</p>
<h2>Takeaways</h2>
<p>The whole point of this exercise was to see if GPT3 can be human-like. But this comparison with Google Search shows that GPT3 is a good language model but it is in no way learning intelligent traits. It can reply to your questions with a precise formulation of the response in terms of grammatical accuracy and consistency with respect to questions, but it is unable to reason about the objects mentioned in the text.</p>
<p>Another result that I firmly believe is that GPT3 learns hard heuristics and memorizes facts. For instance, The New York team has won the World Series titles the most and hence it always returns that they will win any future World Series. Or when objects are compared, the second one is always heavier. Or that whenever we ask what comes before 10^x, 'something and ninety-nine' is always the answer.</p>
<p>Google Search on the other hand gives you correct answers whenever possible but it can not formulate your answers to be more aligned to the question. It will retrieve the information that is closest to what you are looking for but it would rarely be nonsensical.</p>
<p>In other words, GPT3 is just a lossy compression of the data that is not even good at compressing (compression ratio of 2.85) but at the same time has a decent approximate query engine that can parse the input query and formulate an approximately accurate response with grammatical precision. My hypothesis regarding heuristics needs further investigation. If anyone reading this has access to the API and would love to collaborate, please do me a favor.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Thoughts on AI: Will a bias-free AI even be human-like?</title>
        <author>
            <name>Samagra Sharma</name>
        </author>
        <link href="https://www.samagra14.ml/thoughts-on-ai-will-a-bias-free-ai-even-be-human-like.html"/>
        <id>https://www.samagra14.ml/thoughts-on-ai-will-a-bias-free-ai-even-be-human-like.html</id>

        <updated>2021-02-10T00:20:19+05:30</updated>
            <summary>
                <![CDATA[
                    <p>In the past few weeks, there has been much debate around existing biases in our day to day lives and how we should tackle them. This realization was mostly due to the ongoing protests against police brutality in the United States. There were tweets and comments everywhere promoting the idea of a bias-free space. The Artificial Intelligence community wasn’t left untouched, and someone tried the existing language models for their developed biases.</p>

                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>In the past few weeks, there has been much debate around existing biases in our day to day lives and how we should tackle them. This realization was mostly due to the ongoing protests against police brutality in the United States. There were tweets and comments everywhere promoting the idea of a bias-free space. The Artificial Intelligence community wasn’t left untouched, and someone tried the existing language models for their developed biases.</p>

<p>Here is how it turned out:</p>
<blockquote class="twitter-tweet">
<p dir="ltr" lang="en">I don't understand how tech companies can base algorithms on racist/misogynistic data and expect users to mitigate the biases on their own. See this whole thread <a href="https://t.co/BEzTfTmLvC">https://t.co/BEzTfTmLvC</a> <a href="https://t.co/S1lIUQMxgB">pic.twitter.com/S1lIUQMxgB</a></p>
— Ellen K. Pao (@ekp) <a href="https://twitter.com/ekp/status/1271144700896964608?ref_src=twsrc%5Etfw">June 11, 2020</a></blockquote>
<p>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8">This</script>
</p>
<p id="3fca" class="gx gy gg gz b ha pn gi hb hc po gl hd he pp hf hg hh pq hi hj hk pr hl hm ho dg fy" data-selectable-paragraph="">Well, this certainly l<span id="rmm">o</span>oks bad and needs to be tackled. What followed was a series of comments and tweets where people promoted the importance of having a bias-free dataset/algorithm for an intelligent agent. This series of events made me think about a much subtle concept. Aren’t these biases what make us human? Think of it this way; when you write computer programs, you tend to create modules, functions, and objects. All of these constructs are mostly a form of abstraction. We came up with these abstractions to make programming easier for us. We don’t always need to build solutions in a bottom-up manner. These abstractions make it easier for us as programmers to reuse structures that we have spent hours creating and to build on top of them. Don’t you think these biases play a similar role in the context of human reasoning? We don’t have to create a Bayesian Inference loop for every decision we make, we just have preferences, and we tend to choose on them.</p>
<p id="352e" class="gx gy gg gz b ha pn gi hb hc po gl hd he pp hf hg hh pq hi hj hk pr hl hm ho dg fy" data-selectable-paragraph="">Let me indulge you in a thought experiment. This experiment is a slight modification of the infamous trolley experiment. Imagine you are standing at a divergence of train tracks near a controller, which changes the train’s path. In this hypothetical scenario owing to certain unknown events, on one route lies your beloved parent tied to the track and on the other route lie any five participants from the 1927 Solvay Conference. For me, they are Einstein, Bohr, Heisenberg, Haber, and Madam Curie. The scenario is such that you have got just a minute to decide who to save. What will you do?</p>
<figure class="post__image post__image--center" ><img loading="lazy" src="https://miro.medium.com/max/640/0*0WL7k2FF5tnJdQmO.jpg" data-is-external-image="true"  alt="Image for post" width="303" height="160">
<figcaption >
<figure class="fe pj pk pl pm hp fj fk paragraph-image">
<figcaption class="pu pv oa fj fk pw px au b av aw bs" data-selectable-paragraph="">The trolley problem is a thought experiment in ethics modeling an ethical dilemma. It is generally considered to represent a classic clash between two schools of moral thought, utilitarianism and deontological ethics.</figcaption>
</figure>
</figcaption>
</figure>
<p id="4562" class="gx gy gg gz b ha pn gi hb hc po gl hd he pp hf hg hh pq hi hj hk pr hl hm ho dg fy" data-selectable-paragraph="">I am pretty sure that you won’t be doing a full-blown decision-theoretic analysis of the scenario. If you are anything like me, we would most certainly be living in a pre-Quantum Mechanical/Relativistic world with all the pioneers dead. This decision would have been a result of a combination of biases, such as the proximity bias or the self-serving bias. And obviously, a rational agent operating the levers would have saved the five leading scientists owing to their expected utility in the world.</p>
<p id="9772" class="gx gy gg gz b ha pn gi hb hc po gl hd he pp hf hg hh pq hi hj hk pr hl hm ho dg fy" data-selectable-paragraph="">The idea that I want to push forward is the fact that these biases are a fundamental part of our decision engine. Our experiences manifest themselves in the form of biases. Every single one of us has their version of a subjective reality where these biases have created heuristics for quick decision making. As in the case of physical responses, our body builds heuristics at the spinal cord that help us perform immediate reactions to external stimuli; these biases are the heuristics for a fast decision engine.</p>
<p id="cf0d" class="gx gy gg gz b ha pn gi hb hc po gl hd he pp hf hg hh pq hi hj hk pr hl hm ho dg fy" data-selectable-paragraph="">When we argue about removing these biases from intelligent systems, aren’t we pushing them away from human centrism? Do we want them to have a knowledge base reflecting the real world or an ideal world far from reality? Do we want them to make humane decisions, or do we want them to make idealistic utility maximising choices? I don’t think putting curtains on the harsh reality is a wise choice given the final test of performance for these agents would be in the real world. Maybe we can attach a sense of morality to the agents so that they are aware of the existing biases and can still judge the moral correctness of their decisions and opinions.</p>
            ]]>
        </content>
    </entry>
</feed>
